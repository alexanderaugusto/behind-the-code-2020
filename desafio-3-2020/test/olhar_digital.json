[
  {
    "title": "Cientistas criam programa poderoso que aprimora detecção de galáxias",
    "author": "Victor Pinheiro",
    "body": "Pesquisadores da Universidade da Califórnia Santa Cruz (UCSC) desenvolveram um poderoso programa de computador capaz de analisar dados detalhados de imagens astronômicas e ajudar cientistas a identificar e classificar galáxias e estrelas em grandes conjuntos de informações. Batizado de Morpheus, a ferramenta corresponde a uma estrutura de deep learning (aprendizado profundo) e combina uma série de tecnologias de inteligência artificial desenvolvidas para diferentes aplicações, como reconhecimento de fala e imagens. Segundo os cientistas, o recurso permite detectar e fazer a classificação morfológica de objetos astronômicos em um nível de granularidade sem precedentes. Outras soluções já usam tecnologia de deep learning para classificar galáxias, mas esses programas normalmente exigem que os pesquisadores observem a existência de um objeto para, na sequência, alimentar o algoritmo com novas imagens para identificar a galáxia correspondente. Já o modelo dos cientistas da UCSC usa como recurso os dados de imagens originais, e é capaz de identificar diferentes elementos de uma mesma galáxias.  Classificação morfológica do Morpheus para região captada na imagem do Hubble Legacy Fields (Foto: Ryan Hausen/Phys.org) “O Morpheus descobre as galáxias para você e faz [essa análise] pixel por pixel. Ele pode lidar com imagens muito complicadas: você pode ter uma [estrutura] esferoidal ao lado de um disco, por exemplo. Para um disco com um objeto central, [o programa] classifica esse objeto separadamente.\", disse Brant Robertson, professor de astronomia e astrofísica da UCSC e um dos criadores do programa, ao Phys.org. Para treinar o algoritmo de deep learning, os pesquisadores usaram informações de um estudo de 2015, no qual um grupo de astrônomos classificou cerca de 10 mil galáxias com a ajuda de dados do telescópio Hubble. Eles então aplicaram o Morpheus aos dados da imagem do Hubble Legacy Fields, uma figura que retrata dezenas de milhares de galáxias observadas durante vários anos. Quando o Morpheus processa uma imagem de uma área do céu, o programa cria novos grupos de figuras da área capturada, e classifica todos os objetos com código de cores de acordo com sua morfologia. Assim, é possível separar os objetos astronômicos do fundo da imagem e identificar estrelas e diferentes tipos de galáxia. Com ajuda do supercomputador da UCSC, o Morpheus gera rapidamente a análise pixel por pixel para todo o conjunto de dados, assim como estabelece um nível de precisão para cada classificação.  Robertson acredita que o principal impacto da nova tecnologia pode ser o aprimoramento da automatização de processos de pesquisa astronômica. Para ele, isso é essencial à medida que o volume de informações à disposição de cientistas cresce. O pesquisador cita como exemplo a pesquisa Legacy Survey of Space and Time (LSST), que será conduzida no Observatório Vera Rubin, ainda em construção no Chile. A expectativa é que o estudo de larga escala possa gerar enormes quantidades de dados de imagens, e Robertson está envolvido no planejamento de como usar essas informações para investigar a formação e a evolução de galáxias. \"Imagine se você pedisse a astrônomos que eles classifiquem bilhões de objetos. Como eles poderiam fazer isso? Agora, poderemos classificar automaticamente esses objetos e usar essas informações para aprender sobre a evolução das galáxias\", disse Robertson, em referência ao uso do Morpheus. O programa de computador foi descrito em um artigo publicado na revista científica Astrophysical Journal Supplement Series. Os cientistas também divulgaram o código do software publicamente. Fonte: Phys.org",
    "type": "article",
    "url": "https://olhardigital.com.br/ciencia-e-espaco/noticia/cientistas-criam-programa-poderoso-que-aprimora-deteccao-de-galaxias/100683"
  }

{
    "title": "Como a inteligência artificial pode mudar o cenário de oferta de crédito",
    "author": "Jorge Vargas Neto",
    "body": "A Inteligência Artificial (IA) ganhará cada vez mais espaço em todos setores da sociedade, como saúde, economia, indústria e educação. Segundo dados do Sebrae, a IA deve movimentar, em 2018, R$ 730 milhões, no Brasil. Em relatório publicado no dia 3 deste mês, a Organização Mundial do Comércio (OMC) afirmou que tecnologia e inovação vão aumentar o comércio global entre 1,8 e 2 pontos percentuais, por ano, até 2030. O diretor da organização, Roberto Azevêdo, indicou que avanços como blockchain, inteligência artificial e internet das coisas vão mudar mercado. No mercado financeiro, é muito debatido o uso da IA e como a tecnologia pode agregar aos serviços prestados. O assunto vem mobilizando especialistas do Fórum Econômico Mundial que apontou em seu relatório que a IA vai revolucionar o setor, ao introduzir novos modelos de atuação e promover a redução de custos. É importante esclarecer, porém, que a IA propriamente dita, que significa a capacidade de produzir pensamento próprio, também chamada de singularidade, ainda não existe no setor financeiro. O que há, e vem crescendo muito, é o machine learning, ou aprendizado de máquina, que tem duas modalidades: a tradicional e o chamado deep learning (aprendizado profundo). O machine learning permite basicamente ao computador processar informações e com isso gerar modelos estatísticos que vão se aprimorando, conforme são acrescentados mais dados ao sistema. É algo extremamente valioso para o mercado financeiro. Tanto para o crédito, quanto em operações financeiras envolvendo investimento em ações e derivativos, entre outros. Já o deep learning consiste no desenvolvimento de sistemas que absorvem um número alto de dados, de forma que ele comece a criar associações a partir das informações. A aplicação mais conhecida desse modelo é o Google Fotos, que hoje já reconhece, e processa, as imagens com gatos a partir da busca pelo termo \"gatos\" em seu banco de fotos. Feitos esses esclarecimentos, vamos usar o termo inteligência artificial para descrever as aplicações de machine learning e deep learning. Volume dos dados A IA permite o processamento maior de volume de dados, além de mais camadas de informações, de modo a tornar a previsão de riscos de crédito mais precisa em tempo real. Ou seja, a aplicação de algoritmos de aprendizado de máquina aos dados individualizados dos clientes, combinada a fundamentos de boa gestão de riscos, poderá otimizar a identificação de melhores estratégias no gerenciamento de risco de crédito. Dessa forma, é possível acessar novos mercados hoje não atendidos, bem como ter melhor acurácia na concessão. Com isso, poderemos gerar maior concorrência no setor e potencialmente taxas mais baixas aos tomadores de financiamento. Na análise de crédito, é possível introduzir a IA no processamento de dados de relacionamento com clientes, como saldo pendente, data de pagamento de parcelas, dias de atraso, nível de endividamento em relação ao faturamento e comprometimento. Assim, a tecnologia é combinada com dados de agência de crédito padrão – como Serasa e Boa Vista. Isso permite melhor individualização e precificação do risco de crédito dos clientes, tendo como consequência uma oferta de produtos financeiros mais personalizados e maximização dos resultados da oferta de referidos produtos. A inteligência artificial é hoje um dos campos que atrai mais investimentos de bancos e capitalistas de risco em geral. Apesar de ser relativamente recorrente ao longo dos últimos 20 anos, a sua aplicação vem evoluindo gradativamente no exterior, principalmente nos Estados Unidos, Europa e China. Atualmente, a maior parte dos grandes bancos estrangeiros e das empresas de tecnologia de grande porte possui áreas dedicadas a aplicação dessas tecnologias a serviços bancários, entre elas a Amazon Lending, AntFinancial, Tencent e Facebook Payments. Grande potencial No Brasil, o tema está em franca expansão, mas ainda não é aplicado em grande escala, havendo ainda um grande potencial de desenvolvimento e crescimento no mercado. A perspectiva para os próximos anos é que o crescimento da capacidade computacional, especialmente a partir da chegada dos processadores quânticos, permitirá o processamento de um conjunto de dados ainda maior, o que possibilita a individualização dos riscos de cada cliente e melhora da precisão dos modelos preditivos para muitas aplicações, como o gerenciamento de riscos de cartões de crédito em tempo real e outros. A tendência é que, com a presença maior da IA no setor financeiro, surjam modelos cada vez mais precisos e, por consequência, melhores produtos financeiros disponíveis à população. É importante ressaltar que a expansão da tecnologia fez com que surgisse a discussão sobre como regulamentar as aplicações da IA. A Ordem dos Advogados do Brasil (OAB), por exemplo, anunciou em julho a criação de um grupo para debater a regulamentação do uso da IA no Direito.",
    "type": "article",
    "url": "https://olhardigital.com.br/colunistas/jorge_vargas_neto/post/como_a_inteligencia_artificial_pode_mudar_o_cenario_de_oferta_de_credito/78999"
  }

{
    "title": "Inteligência artificial ajuda a Nasa a projetar novos trajes espaciais",
    "author": "Rafael Rigues",
    "body": "A Nasa está usando inteligência artificial para auxiliar no design de sua nova geração de trajes espaciais. Batizados de Extravehicular Mobility Unit (Unidade de Mobilidade Extraveicular, xEMU), eles são o primeiro redesenho do equipamento em quase 40 anos, e serão usados nas futuras missões tripuladas à Lua no Programa Artemis. A IA está sendo usada para projetar partes do \"sistema portátil de suporte à vida\", a \"mochila\" que os astronautas carregam nas costas, que regula sua temperatura corporal, fornece energia e abriga o sistema de comunicação do traje. Como este é o primeiro uso da tecnologia, por enquanto ela não está sendo usada em sistemas críticos. \"Ainda estamos no início de um programa piloto, então não estamos usando ela em nada que possa causar uma falha catastrófica\", diz Sean Miller, engenheiro mecânico da Nasa. Em vez disso, ela está sendo usada para projetar braçadeiras e suportes que irão sustentar os sistemas que mantém os astronautas vivos. Pode não ser um trabalho glamouroso, mas graças à IA a Nasa conseguiu reduzir o peso de alguns componentes em até 50%.  O xEMU é o traje que será usado nas missões do programa Artemis, que pretende levar uma mulher à Lua. Foto: Loren Grush / The Verge. \"Quando a Nasa define os requisitos para um sistema de pouso tripulado, ela aloca um certo peso para cada componente imaginável, que temos que respeitar\", diz Miller. \"Então, qualquer lugar onde possamos economizar mesmo um décimo de grama nos ajuda a chegar mais perto do limite de peso da missão\" Segundo Jesse Coors-Blankenship, Vice-Presidente de Tecnologia da PTC, empresa que desenvolveu o software usado pela Nasa, \"consideramos a IA como uma tecnologia que pode fazer algo mais rápido e melhor do que um humano treinado pode fazer. Algumas das tecnologias são coisas com as quais os engenheiros já estão acostumados, como simulação e otimização estrutural. Mas com a IA, podemos fazer isso mais rapidamente\". O software da PTC combina abordagens como redes generativas adversariais e algoritmos genéticos. Na primeira dois algoritmos \"competem\" na criação de um componente, e o melhor resultado é usado como base para uma geração seguinte, sucessivamente até que o objetivo desejado seja atingido. Já os algoritmos genéticos reproduzem a seleção natural. Eles produzem vários designs, combinam suas melhores características, analisam os resultados e repetem o processo. Segundo Jesse Craft, engenheiro sênior da Jacobs, empresa do Texas que foi contratada pela Nasa para auxiliar no projeto do xEMU, \"o processo iterativo das máquinas é 100 ou 1.000 vezes mais rápido do que poderíamos fazer sozinhos, e o resultado é uma solução que é idealmente otimizada para nossos requisitos\". Fonte: Wired",
    "type": "article",
    "url": "https://olhardigital.com.br/ciencia-e-espaco/noticia/inteligencia-artificial-ajuda-a-nasa-a-projetar-novos-trajes-espaciais/102772"
  }

{
    "title": "Inteligência Artificial da IBM consegue prever câncer de mama",
    "author": "Clara GuimarÃ£es",
    "body": "Pesquisadores da IBM desenvolveram um modelo de Inteligência Artificial (IA) que pode prever câncer de mama maligno com até 1 ano de antecedência e com 87% de precisão. Embora já existam métodos de previsão de IA que dependem de imagens de mamografia e registros médicos, a IBM destaca-se por usar ambos - e obter, potencialmente, um resultado mais confiável. A abordagem da IBM treina a IA com imagens de mamografia anônimas ligadas a biomarcadores  e dados clínicos, permitindo a criação de um algoritmo de alta precisão, Ele consegue, portanto, reduzir a taxa de erro ao analisar além da imagem, vendo coisas como nível de ferro e funcionamento da tireoíde. Contudo, você provavelmente não gostaria de confiar apenas no algoritmo para fazer previsões, mas ele serviria como um bom \"segundo conjunto de olhos\", diz a IBM. Ele poderia verificar o prognóstico de um radiologista e reduzir as chances de os pacientes serem enviados para exames de acompanhamento desnecessários. Isso é muito útil em países com escassez de médicos ou em qualquer situação em que não há muito tempo para exames humanos. A forma de previsão da IBM pode não ser a mais rápida - considerando que o MIT lançou um projeto similar capaz de identificar o câncer com até 5 anos de antecedência-, mas ela está apostando em uma visão mais completa da situação, que não confie apenas em imagens e que atue em conjunto com médicos. De qualquer forma, há uma possibilidade real de que mais pacientes com câncer de mama iniciem o tratamento antes que o primeiro tumor apareça. Via: Engadget      ",
    "type": "article",
    "url": "https://olhardigital.com.br/noticia/inteligencia-artificial-da-ibm-consegue-prever-cancer-de-mama/87030"
  }

{
    "title": "Nova teoria diz que passado, presente e futuro coexistem",
    "author": "Renato Mota",
    "body": "Diferente de um \"rio que corre\", o tempo não se comporta da forma como o percebemos. Passado, presente e futuro existem simultaneamente, mas em dimensões diferentes. Esse é o fundamento por trás do conceito do \"Bloco universal\", defendido pelo professor associado de filosofia do Instituto de Tecnologia de Massachusetts (MIT), Bradford Skow. Em seu livro \"Objective Becoming\", Skow detalha essa visão, que defende que tempo deve ser considerado como uma dimensão do espaço-tempo, como sustenta a teoria da relatividade. Dessa maneira, ele não “passa” por nós, mas faz parte do tecido maior do universo – ao invés de ser algo que se move dentro dele. \"A teoria do universo em bloco diz que você se espalha pelo tempo, da mesma maneira como se espalha no espaço\", explica Skow. \"Não estamos localizados em um único momento\". Ou sejam, eventos ocorrem, pessoas envelhecem e assim por diante. \"As coisas mudam\", afirma o professor de filosofia. Mas o passado não “desaparece”, ele simplesmente existe em diferentes partes do espaço-tempo. Esse pensamento possui uma base na teoria do espaço e do tempo unificados, proposta por Albert Einstein em 1915. Em sua teoria geral da relatividade, o físico alemão propõe que o espaço-tempo toma forma de maneira múltipla ou contínua, que podem ser visualizados como um espaço vetorial quadridimensional. \"A distinção entre passado, presente e futuro é apenas uma ilusão teimosamente persistente\", afirmou Einstein. No livro, Skow considera diversas explicações alternativas para como o tempo se comporta, mas se diz mais impressionado com a teoria dos \"holofotes em movimento\", que coloca passado e o futuro em pé de igualdade com o presente. No entanto, a teoria sustenta que apenas um momento de cada vez está absolutamente presente, e esse momento continua mudando, como se um holofote estivesse se movendo sobre ela. As experiências que você teve há um ou dez anos ainda são igualmente reais, afirma Skow; eles são apenas \"inacessíveis\" porque agora você está em uma parte diferente do espaço-tempo (o que inviabiliza, infelizmente, viagens no tempo). Via: MIT News",
    "type": "article",
    "url": "https://olhardigital.com.br/ciencia-e-espaco/noticia/nova-teoria-diz-que-passado-presente-e-futuro-coexistem/97786"
  }

{
    "title": "O Futuro cada vez mais perto",
    "author": "Wagner Sanchez",
    "body": "Estamos presenciando uma fusão de tecnologias, exponenciais ocasionando mudanças radicais nos formatos das empresas, nas funções dos profissionais dentro das empresas, na forma como as pessoas compram, se comunicam, estudam, enfim, estamos sentindo em nosso dia a dia experiências nunca antes vividas. Neste contexto, temos um processo que estamos chamando de cognificação, que resumidamente significa adicionar inteligência a qualquer dispositivo que interagimos. Hoje, temos diversos aparelhos com alguma inteligência artificial, mas a tendência é que tenhamos uma grande oferta de sistemas inteligentes on demand para utilizarmos nas mais diversas funções, nos ajudando a resolver problemas de forma mais assertiva e rápida. Podemos até comparar com a primeira revolução industrial que foi potencializada por uma força artificial chamada eletricidade. Ela nos deu acesso a diversos dispositivos indispensáveis hoje em dia, tais como: geladeira, televisão, computadores etc. O que estamos presenciando é algo semelhante em relação às inteligências artificiais. Estamos no princípio de um novo paradigma que pode proporcionar uma nova etapa na evolução da humanidade. A era da cognificação pode significar uma mudança no modo como resolvemos os problemas atualmente: a inteligência artificial estará muito mais presente em nossas vidas, convivendo com a inteligência natural em busca de novas soluções. Dentro deste novo cenário, o que antes era moldado pela eletricidade, será formatado pela cognificação, ainda mais quando outras tecnologias estão se fundindo para proporcionar novas soluções, tais como: Big Data, IoT, Neurohacking, Prototipação 3D, Nanotecnologia, Design Thinking, Deep Learning, Cloud Computing etc. As evoluções não irão cessar, podem até diminuir a velocidade, mas continuarão a substituir atividades repetitivas e que demandem pouca improvisação. Assim, o profissional que deseja surfar a melhor onda de sua área, deverá estar pronto para colaborar com o desenvolvimento e operação das máquinas inteligentes que estão chegando e que ainda irão chegar no mercado. Por que não se tornar o “treinador” ou o “desenvolvedor” das inteligências artificiais? Neste contexto da quarta revolução industrial, podemos, seguramente, prever algumas disrupções profundas, como, por exemplo, na área da saúde. A biomedicina deverá passar por fortes evoluções, onde as tecnologias exponenciais já citadas irão desempenhar boa parte das funções que compete aos humanos hoje, cabendo ao ser humano as funções menos repetitivas e com maior demanda nas tomadas de decisões inéditas com pouco histórico. A evolução dos equipamentos médicos está a todo vapor com a inserção da inteligência artificial e uma maior precisão do escaneamento humano, proporcionando diagnósticos com mais precisão e rapidez, ainda mais se pensarmos que todo diagnóstico, prevenção e tratamento poderão ter a colaboração de uma rede mundial de inteligências artificiais com machine learning interligada em cloud, trocando milhões e milhões de informações por segundo, onde tudo será compartilhado e aprendido. Um diagnóstico realizado aqui no Brasil poderá ajudar imediatamente um outro diagnóstico na Índia e assim por diante. Com isso, as tomadas de decisões devem alcançar um novo patamar de eficiência e rapidez. A substituição completa dos profissionais pela máquina não irá acontecer. Vamos vivenciar um cenário onde as tecnologias exponenciais estarão a cada dia mais próximas aos profissionais de todas as áreas como um grande aliado na solução de problemas utilizando Data Science, Inteligência Artificial, Cognificação, Optical Devices, Biopotenciais, Wearables, Realidade Virtual, Biomateriais, Robótica, Cibersegurança, Neurohacking, IoT - Internet of Things, Análise Preditiva e Data Mining, tudo em prol de maior assertividade e velocidade, melhorando a qualidade de vida das pessoas.",
    "type": "article",
    "url": "https://olhardigital.com.br/colunistas/wagner_sanchez/post/o_futuro_cada_vez_mais_perto/78972"
  }

{
    "title": "Os riscos do Machine Learning",
    "author": "Wagner Sanchez",
    "body": "A inteligência artificial, e consequentemente o machine learning, vem sendo objeto de estudo desde a década de 1940, quando surgiram os primeiros neurônios artificiais. Desde então, os desenvolvedores de códigos computacionais vêm aprimorando suas técnicas com o objetivo de simular a inteligência humana e a nossa forma de adquirir conhecimento. Sabemos que a maioria de nossas habilidades são aprendidas ao longo da infância e da juventude até a vida adulta como por exemplo: falar, andar, escrever, dirigir automóveis, praticar esportes, entre outros. Aprendemos olhando, escutando, interagindo e praticando algo que nos interessa ou que nos é apresentado como necessidade ou como fonte de prazer. Nos algoritmos computacionais, o conceito atual de aprendizado de máquina é semelhante: treinamos as inteligências artificiais sempre que interagimos com uma. As redes neurais artificiais são desenvolvidas para se comportar como uma esponja de absorção de conhecimento, tal qual os bebês quando estão se desenvolvendo, todos sedentos por conhecimentos. Outra forma de ensinarmos uma máquina é apresentando a ela grandes quantidades de informações em forma de soluções desenvolvidas por seres humanos. Desta forma, as redes neurais artificiais aprendem como resolvemos problemas e passam a seguir os nossos padrões e, posteriormente, podem adotar seus próprios.  Um exemplo interessante deste tipo de aprendizado é a experiência da IBM com o Watson. Ele foi ensinado a criar trailer de filmes através da comparação entre as produções cinematográficas e seus respectivos trailers. Watson “entendeu” como os seres humanos os criam: qual o padrão que os cortes devem seguir de acordo com as expressões faciais dos atores, qual o volume da trilha sonora, quanto tempo deve ter, entre outras características. Neste contexto, é importante ressaltar a importância da qualidade das informações apresentadas às inteligências artificiais e, principalmente, a idoneidade das pessoas que estão treinando as redes neurais artificiais, pois corre-se o risco de se desenvolver inteligências artificiais preconceituosas, com tendências maléficas. O chatbot Tay, da Microsoft, é um exemplo de como pessoas mal-intencionadas podem ensinar coisas ruins para uma inteligência artificial. Tay foi desenvolvido para interagir e aprender com jovens entre 18 e 24 anos. Porém, alguns delinquentes bombardearam o chatbot com mensagens em prol do nazismo. Com isso, Tay começou tuitar espontaneamente mensagens e imagens a favor de Hitler, como o exemplo a seguir:   Outro exemplo mais recente foi o desafio aceito por um time do laboratório de mídia do MIT - Instituto de Tecnologia de Massachusetts. Três pesquisadores desenvolveram uma inteligência artificial psicopata, deram a ela o nome de Norman, em homenagem a Norman Bates, personagem do filme Psicose de 1960. Para tanto, foram programadas duas redes neurais com o objetivo de interpretar imagens. Para o treinamento destas duas redes foram usadas imagens com dois padrões diferentes. Para a rede Norman, apresentou-se imagens violentas de cenas de mortes e para a outra rede, foram utilizadas imagens comuns da internet. O resultado não podia ser diferente. Norman aprendeu a ter comportamento de um psicopata na interpretação de imagens e a outra rede teve uma atuação bem mais amena. Para comprovar o experimento, os pesquisadores utilizaram o teste de Rorschach, usado por psicólogos para a avaliação da saúde mental e emocional dos pacientes. Foram apresentados os borrões de Rorschach às duas redes neurais e o resultado foi surpreendente. Enquanto a rede neural normal interpretava as figuras como sendo pássaros inofensivos, Norman interpretava as mesmas imagens como sendo pessoas mortas de forma violenta. (Este experimento pode ser acessado na íntegra em http://norman-ai.mit.edu/) Estes dois exemplos ilustram a importância de estarmos atentos ao treinamento das inteligências artificiais que estão presentes em nosso dia a dia: nos atendimentos em hospitais, na área da segurança, na educação, em finanças, em entretenimento etc. Dependendo das informações expostas e das pessoas envolvidas na transmissão de conhecimentos para as redes neurais artificiais, podemos ter surpresas desagradáveis em um futuro muito próximo.",
    "type": "article",
    "url": "https://olhardigital.com.br/colunistas/wagner_sanchez/post/os_riscos_do_machine_learning/80584"
  }
]